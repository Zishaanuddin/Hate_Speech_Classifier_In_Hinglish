{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hate_Speech_Case_Study_Custom_Stacking_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkMClbyQ7qHU",
        "outputId": "980f6ffb-6b7a-4703-8d5d-9e6cba3306c9"
      },
      "source": [
        "pip install googletrans==3.1.0a0 --upgrade --quiet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 61kB 4.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 17.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw09MBzAooE-",
        "outputId": "951ac2f0-60b6-4bc0-f138-5dc3a18960e6"
      },
      "source": [
        "pip install catboost --upgrade --quiet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 69.2MB 41kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4YQXzXIky4B",
        "outputId": "99c0d92e-5ce9-474d-9da7-820b75eadc1f"
      },
      "source": [
        "# importing the important library\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk \n",
        "import re\n",
        "import seaborn as sns\n",
        "from googletrans import Translator, constants\n",
        "from pprint import pprint\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.utils import resample"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "XRxRp0i9ky4K",
        "outputId": "27aaf122-5508-4ed5-9593-6eb94a54a234"
      },
      "source": [
        "project_data = pd.read_csv('HOT_Dataset_modified.csv', encoding='utf-8', header=None)\n",
        "project_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>@saud5683 @Mutayyab420 @shivang598 @Ranask35 @...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Banti hai empowered woman, feminism pe gyan pe...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>RT @kim_jong_korea: @updatingwait @Acutereply ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0                                                  1    2   ...   16   17   18\n",
              "0  0.0  @saud5683 @Mutayyab420 @shivang598 @Ranask35 @...  NaN  ...  NaN  NaN  NaN\n",
              "1  NaN                                                NaN  NaN  ...  NaN  NaN  NaN\n",
              "2  2.0  Banti hai empowered woman, feminism pe gyan pe...  NaN  ...  NaN  NaN  NaN\n",
              "3  NaN                                                NaN  NaN  ...  NaN  NaN  NaN\n",
              "4  2.0  RT @kim_jong_korea: @updatingwait @Acutereply ...  NaN  ...  NaN  NaN  NaN\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QueiE0wKky4M"
      },
      "source": [
        "project_data = project_data.dropna(how='all')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw93GXuuky4N"
      },
      "source": [
        "project_data = project_data.iloc[0::, 0:2]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwkvMcky4N",
        "outputId": "74cdc965-808f-4115-db30-df8531b871e1"
      },
      "source": [
        "project_data[0].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0    1765\n",
              "0.0    1121\n",
              "1.0     303\n",
              "Name: 0, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-MvZsK2ky4O"
      },
      "source": [
        "project_data.columns = ['label', 'tweet']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH9Zzf5Hky4P",
        "outputId": "69220457-c258-4f38-d014-c08cb6206ca7"
      },
      "source": [
        "project_data.isnull().any()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label    False\n",
              "tweet    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1NeOzKNky4k"
      },
      "source": [
        "## Function for Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIzUnsPGky4l"
      },
      "source": [
        "def userid(tweet):\n",
        "    ''' This function calculates the number of userids in the tweets'''\n",
        "    count = 0\n",
        "    for i in tweet.split():\n",
        "        if i[0] == '@':\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def profanity_vector(tweet):\n",
        "    \n",
        "    ''' This functions calculates the profanity vector for a given tweet '''\n",
        "    \n",
        "    bad_words = pd.read_csv('Hinglish_Profanity_List.csv', engine='python', header=None)\n",
        "    bad_words.columns = ['Hinglish', 'English', 'Level']\n",
        "    hinglish = bad_words['Hinglish'].values\n",
        "    level = bad_words['Level'].values\n",
        "    PV = [0] * len(level)\n",
        "    for word in tweet.split():\n",
        "        if word in hinglish:\n",
        "            idx = np.where(hinglish == word)\n",
        "            PV[level[idx][0]] = 1\n",
        "    return PV\n",
        "\n",
        "def translation(tweet):\n",
        "    \n",
        "    ''' This function translates the hinglish tweet into english '''\n",
        "    translator = Translator()\n",
        "    trans = translator.translate(tweet)\n",
        "    trans_tweet = trans.text\n",
        "    \n",
        "    return trans_tweet.lower()\n",
        "\n",
        "def stopword(data):\n",
        "    \n",
        "    ''' This function removes the stopwords from the given sentence'''\n",
        "    clean = []\n",
        "    stop_words = set(STOPWORDS)\n",
        "    \n",
        "    for tweet in data:\n",
        "        sentence = []\n",
        "        for word in tweet.split():\n",
        "            if word not in stop_words:\n",
        "                sentence.append(word)\n",
        "        clean.append(sentence)\n",
        "    return clean\n",
        "\n",
        "def Lemmatizer(tweet):\n",
        "    \n",
        "    ''' This function uses NLTK lemmatization method and clean the sentence'''\n",
        "    lemma = []\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    for word in tweet:\n",
        "        sentence = []\n",
        "        for i in word:\n",
        "             sentence.append(lemmatizer.lemmatize(i))\n",
        "        lemma.append(' '.join(sentence))\n",
        "    return lemma\n",
        "\n",
        "def SID(tweet):\n",
        "    \n",
        "    ''' This function calculates the NLTK sentiments and return the negative, neutral, postive and compound values'''\n",
        "    negative = []\n",
        "    neutral = []\n",
        "    positive = []\n",
        "    compound = []\n",
        "    \n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    sentiment_score = sid.polarity_scores(tweet)\n",
        "    \n",
        "    negative.append(sentiment_score['neg'])\n",
        "    neutral.append(sentiment_score['neu'])\n",
        "    positive.append(sentiment_score['pos'])\n",
        "    compound.append(sentiment_score['compound'])\n",
        "    \n",
        "    return negative, neutral, positive, compound\n",
        "\n",
        "def imp_features(data, y, keep):\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n",
        "    rf.fit(data, y)\n",
        "    imp_feature = np.argsort(rf.feature_importances_)[::-1]\n",
        "\n",
        "    return imp_feature[:keep]\n",
        "\n",
        "\n",
        "def cleaning(data):\n",
        "    \n",
        "    ''' This functions clean the input text'''\n",
        "    \n",
        "    user_ids = []\n",
        "    clean_data_hinglish = []\n",
        "    clean_translated_data = []\n",
        "    prof_vector = []\n",
        "    \n",
        "    for tweet in tqdm(data):\n",
        "        userids = userid(tweet)\n",
        "        clean_text = []\n",
        "        tweet = re.sub(r'\\\\n', ' ', tweet)  # replacing '\\\\n' with a space\n",
        "        tweet = re.sub(r',', ' ', tweet)    # replacing ','  with a space\n",
        "        tweet = re.sub(r'RT|rt', '', tweet)\n",
        "        \n",
        "        for word in tweet.split():\n",
        "            if word[0] == '@':              # removing user_ids \n",
        "                clean_word = re.sub(word, 'username', word)\n",
        "            else:\n",
        "                clean_word = word.lower()       # lowercase all the words\n",
        "                clean_word = re.sub(r'^#\\w+', ' ', clean_word)\n",
        "                #clean_word = re.sub(r'^\\\\[a-z0-9].*\\\\[a-z0-9{3}+]*[^\\\\n]$', '', clean_word)   # removing emotions in unicode\n",
        "                clean_word = re.sub(r'\\\\', ' ', clean_word)\n",
        "                clean_word = re.sub(r'^https:[\\a-zA-Z0-9]+', '', clean_word)              # replacing url link with 'url'\n",
        "                #clean_word = re.sub(r'[^a-z].\\w+', '', clean_word)           # removing evering thing except a-z\n",
        "                clean_word = re.sub(r'[!,.:_;$%^\\'\\#\"&]', '', clean_word)\n",
        "                clean_text.append(clean_word)\n",
        "                \n",
        "        clean_text = (' ').join(clean_text)\n",
        "    \n",
        "        PV = profanity_vector(clean_text)  # calling profanity_vector function\n",
        "        translated_tweet = translation(clean_text)  #calling translated_tweet function\n",
        "        \n",
        "        user_ids.append(userids)\n",
        "        clean_data_hinglish.append(clean_text)\n",
        "        clean_translated_data.append(translated_tweet)\n",
        "        prof_vector.append(PV)\n",
        "        \n",
        "        \n",
        "    clean_data_hinglish = np.asarray(clean_data_hinglish)\n",
        "    user_ids = np.asarray(user_ids).reshape(-1,1)\n",
        "    prof_vector = np.asarray(prof_vector)\n",
        "    clean_translated_data = np.asarray(clean_translated_data)\n",
        "\n",
        "        \n",
        "    return clean_data_hinglish, user_ids, prof_vector, clean_translated_data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjxScdJ7ky4q"
      },
      "source": [
        "### Function for feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK3wvxFPky4r"
      },
      "source": [
        "def feature_process(clean_data_train, clean_data_test, userids_train, userids_test, PV_train, PV_test):\n",
        "    ''' This function except the clean data and return Train and Test dataset after stacking userids, profanity vector, negative sentiment, neutral sentiment, \n",
        "                    positive sentiment, compound sentiment, n-grams and tfidf features'''\n",
        "    \n",
        "    vectorizer = CountVectorizer()\n",
        "    tfidf = TfidfVectorizer()\n",
        "    scaler = MinMaxScaler()\n",
        "    negative_train, negative_test = [], []\n",
        "    neutral_train, neutral_test = [], []\n",
        "    positive_train, positive_test  = [], []\n",
        "    compound_train, compound_test  = [], []\n",
        "\n",
        "    for tweet in clean_data_train:\n",
        "        neg, neu, pos, comp = SID(tweet)\n",
        "        negative_train.append(neg), neutral_train.append(neu), positive_train.append(pos), compound_train.append(comp)\n",
        "    \n",
        "    for tweet in clean_data_test:\n",
        "        neg, neu, pos, comp = SID(tweet)\n",
        "        negative_test.append(neg), neutral_test.append(neu), positive_test.append(pos), compound_test.append(comp)\n",
        "    \n",
        "    clean_data_SW_train = stopword(clean_data_train)\n",
        "    clean_data_SW_test = stopword(clean_data_test)\n",
        "    \n",
        "    clean_data_lemm_train = Lemmatizer(clean_data_SW_train)\n",
        "    clean_data_lemm_test = Lemmatizer(clean_data_SW_test)\n",
        "    \n",
        "    vectorizer.fit(clean_data_lemm_train)\n",
        "    tfidf.fit(clean_data_lemm_train)\n",
        "    \n",
        "    n_grams_train = vectorizer.transform(clean_data_lemm_train)\n",
        "    tfidf_ngrams_train = tfidf.transform(clean_data_lemm_train)\n",
        "    \n",
        "    n_grams_test = vectorizer.transform(clean_data_lemm_test)\n",
        "    tfidf_ngrams_test = tfidf.transform(clean_data_lemm_test)\n",
        "    \n",
        "    negative_train, negative_test = np.asarray(negative_train), np.asarray(negative_test)\n",
        "    neutral_train, neutral_test = np.asarray(neutral_train), np.asarray(neutral_test)\n",
        "    positive_train, positive_test  = np.asarray(positive_train), np.asarray(positive_test)\n",
        "    compound_train, compound_test = np.asarray(compound_train), np.asarray(compound_test)\n",
        "    userids_train = scaler.fit_transform(userids_train)\n",
        "    userids_test = scaler.transform(userids_test)\n",
        "    \n",
        "    train_dataset = hstack((userids_train, PV_train, negative_train, neutral_train, positive_train, compound_train, n_grams_train, tfidf_ngrams_train))\n",
        "    \n",
        "    test_dataset = hstack((userids_test, PV_test, negative_test, neutral_test, positive_test, compound_test, n_grams_test, tfidf_ngrams_test))\n",
        "    \n",
        "    \n",
        "    return train_dataset, test_dataset"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_VWvzb_ky4r"
      },
      "source": [
        "## Spliting Train and Test Dataset into 80:20\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQU4Vvj3ky4r"
      },
      "source": [
        "X = project_data['tweet']\n",
        "y = project_data['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRZn0DENky4s",
        "scrolled": false,
        "outputId": "14dcc39d-2f72-40f1-cc10-2988e28bd318"
      },
      "source": [
        "# calling the cleaning function which returns the values for the train dataset.\n",
        "\n",
        "clean_data_hinglish_train, user_ids_train, prof_vector_train, clean_translated_data_train = cleaning(X_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2551/2551 [15:55<00:00,  2.67it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ7mz_zwky4s",
        "outputId": "0b87d91d-b67f-43b4-a8eb-e7cc52ade281"
      },
      "source": [
        "# calling the cleaning function which returns the values for the test dataset\n",
        "\n",
        "clean_data_hinglish_test, user_ids_test, prof_vector_test, clean_translated_data_test = cleaning(X_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 638/638 [04:02<00:00,  2.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I7TZ7x-ky4s"
      },
      "source": [
        "#Calling feature_process which return the complete train and test dataset.\n",
        "\n",
        "Train, Test = feature_process(clean_translated_data_train, clean_translated_data_test, user_ids_train, user_ids_test, prof_vector_train, prof_vector_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up-YGwFBky4t",
        "outputId": "98e1e244-36c8-4ec3-a53a-bd6b5d2377e1"
      },
      "source": [
        "print('{} is the shape of Train Dataset and {} is the shape of Test Dataset'.format(Train.shape, Test.shape))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2551, 14022) is the shape of Train Dataset and (638, 14022) is the shape of Test Dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATrF_nA8ky4t"
      },
      "source": [
        "def plot_confusion_matrix(test_y, predict_y):\n",
        "    \n",
        "    '''This function returns confusion matrix, precison matrix and recall matrix for 3 class classification'''\n",
        "    \n",
        "    C = confusion_matrix(test_y, predict_y)\n",
        "    print(\"Number of misclassified points \",(len(test_y)-np.trace(C))/len(test_y)*100)\n",
        "    \n",
        "    A =(((C.T)/(C.sum(axis=1))).T)\n",
        "    B =(C/C.sum(axis=0))\n",
        "    \n",
        "    labels = ['Non Offensive', 'Hate Speech', 'Abusive']\n",
        "    cmap=sns.light_palette(\"green\")\n",
        "    \n",
        "    # representing A in heatmap format\n",
        "    print(\"-\"*50, \"Confusion matrix\", \"-\"*50)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"-\"*50, \"Precision matrix\", \"-\"*50)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "    print(\"Sum of columns in precision matrix\",B.sum(axis=0))\n",
        "    \n",
        "    # representing B in heatmap format\n",
        "    print(\"-\"*50, \"Recall matrix\" , \"-\"*50)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.show()\n",
        "    print(\"Sum of rows in Recall matrix\",A.sum(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH3t834KQugZ"
      },
      "source": [
        "## Custom Stacking Classifier:\n",
        "\n",
        "##### a. Splitting the train data into 2 parts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP2DeYJHQsFD",
        "outputId": "1e6806f4-bbdd-4718-cbd0-6a85d0fcd928"
      },
      "source": [
        "# splitting the Actuall Train Dataset into 50:50\n",
        "\n",
        "Train_D1, Train_D2, y_train_D1, y_train_D2 = train_test_split(Train, y_train, test_size=0.5, stratify=y_train, random_state=42)\n",
        "\n",
        "print('{} is the shape of Train Data D1 and {} is the shape of Train data D2.'.format(Train_D1.shape, Train_D2.shape))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1275, 14022) is the shape of Train Data D1 and (1276, 14022) is the shape of Train data D2.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUJqGcwRky4t"
      },
      "source": [
        "##### b. Intializing different type of classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFO8ZbicDjNq"
      },
      "source": [
        "# Initialing Support Vector classifier with Linear Kernel\n",
        "SVC_Linear = SVC(C=100, gamma=\"auto\", kernel = \"linear\", probability=True, random_state=42)\n",
        "\n",
        "# Initializing Support Vector classifier with Poly kernel\n",
        "SVC_Poly = SVC(C = 75, degree = 2, gamma = \"auto\", probability=True, kernel = \"poly\", random_state=42)\n",
        "\n",
        "# Initializing Support Vector classifier with RBF kernel\n",
        "SVC_RBF = SVC(C = 100, degree = 1, gamma = \"auto\", probability=True, kernel = \"rbf\", random_state=42)\n",
        "\n",
        "# Intitializing Logistic Regression Classifier\n",
        "Logistic_Regression = LogisticRegression(C=1, max_iter=500, penalty='l2', random_state=42, n_jobs = -1)\n",
        "\n",
        "# Intitializing KNN Classifier\n",
        "KNN = KNeighborsClassifier(n_neighbors=3) \n",
        "\n",
        "# Intitializing Decision Tree Classifier\n",
        "Decision_Tree = DecisionTreeClassifier(criterion='gini', max_depth=18, min_samples_split=2)\n",
        "\n",
        "# Intitializing Extra_Tree Classifier\n",
        "Extra_Tree = ExtraTreesClassifier(criterion='gini', max_depth=200, n_estimators=500, n_jobs=-1)\n",
        "\n",
        "# Initializing Random Forest classifier\n",
        "Random_Forest = RandomForestClassifier(n_estimators = 500, criterion = \"gini\", max_depth = 10, max_features = \"auto\", n_jobs = -1, random_state = 42)\n",
        "\n",
        "# Initializing XGBOOST  classifier\n",
        "XGB = XGBClassifier(depth=10, learning_rate =0.7, iterations=200, n_jobs=-1,random_state=42)\n",
        "\n",
        "# Initializing CatBoost classifier\n",
        "CatBoost = CatBoostClassifier(depth= 10, learning_rate =0.6, iterations=150, random_state=42, verbose=0)\n",
        "\n",
        "# Ini-tializing Light GBM classifier\n",
        "Light_GBM = LGBMClassifier(max_depth= 10, min_data_in_leaf = 2, num_leaves=50, n_jobs=-1, random_state=42)\n",
        "\n",
        "classifiers_dict = {\"SVC-RBF\": SVC_RBF,\n",
        "               \"XGB\": XGB,\n",
        "               \"SVC_Linear\": SVC_Linear,\n",
        "               \"CatBoost\": CatBoost,\n",
        "               \"Light_GBM\": Light_GBM,\n",
        "               \"Random_Forest\": Random_Forest,\n",
        "               \"Logistic_Regression\": Logistic_Regression,}\n",
        "\n",
        "base_models_list = [SVC_Linear, SVC_RBF, SVC_Poly, XGB, CatBoost, KNN, Decision_Tree, Extra_Tree, Random_Forest, Light_GBM, Logistic_Regression,]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appwp29FqmEB"
      },
      "source": [
        "##### c. Function for the custom stacking classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7HGQ7iBZLGU",
        "outputId": "5dddee67-cbbf-419a-cd4f-b10385a80583"
      },
      "source": [
        "def stacking_classifier(k_model, base_models, meta_model, D1, D2, y_D1, y_D2, test, y_te):\n",
        "  \"\"\"This function performs custom stacking classification as it takes no. of base models, then the output of the base models will the added to the another dataset and that dataset used to train the meta classifier\"\"\" \n",
        "  base_model_fits = []\n",
        "  predictions = pd.DataFrame()\n",
        "  predictions_test = pd.DataFrame()\n",
        "  for i in range(k_model):\n",
        "      base_model = base_models[i]\n",
        "\n",
        "      # Get a random sample with replacement with a size of 1000 from D1 \n",
        "      train_sample, y_sample = resample(D1, y_D1, n_samples=1000, stratify=y_D1, random_state=42)\n",
        "\n",
        "      base_model.fit(train_sample, y_sample) # train the model on sample\n",
        "      base_model_fits.append(base_model) # save the base model\n",
        "\n",
        "  for j in range(k_model): # send D2 to all base models\n",
        "      y_pred = base_model_fits[j].predict_proba(D2) #predict  the probablities of the classes for D2 set\n",
        "      for c in range(len(y_pred[0])):\n",
        "          predictions[f\"{j}{c}\"] = y_pred[0:,c] # store the probablities of each class \n",
        "\n",
        "  for m in range(k_model): # send test data to all base models\n",
        "      y_pred_test = base_model_fits[m].predict_proba(test) #predict  the probablities of the classes for test set\n",
        "      for p in range(len(y_pred_test[0])):\n",
        "          predictions_test[f\"{m}{p}\"] = y_pred_test[0:,p]  # store the probablities of each class\n",
        "        \n",
        "  meta_data_train = hstack((D2, predictions)) # stacking the D2 data and prediction probablities from k base model for meta classifier\n",
        "  meta_data_test = hstack((test, predictions_test)) # stacking the train data and k prediction for test set\n",
        "\n",
        "  \n",
        "  meta_model.fit(meta_data_train, y_D2) # train the meta model on meta data\n",
        "  print(\"F1-Score for {} as Meta classifier and {} number of base model is {}\".format('Logistic_Regression', K+1, np.round(f1_score(y_te, meta_model.predict(meta_data_test), average='macro'), 6)))\n",
        "  #plot_confusion_matrix(y_te, meta_classifier.predict(Test_new))\n",
        "\n",
        "        \n",
        "        \n",
        "for K in range(len(base_models_list)):\n",
        "  stacking_classifier(K+1, base_models_list, Logistic_Regression, Train_D1, Train_D2, y_train_D1, y_train_D2, Test, y_test)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-Score for Logistic_Regression as Meta classifier and 1 number of base model is 0.810756\n",
            "F1-Score for Logistic_Regression as Meta classifier and 2 number of base model is 0.80566\n",
            "F1-Score for Logistic_Regression as Meta classifier and 3 number of base model is 0.80566\n",
            "F1-Score for Logistic_Regression as Meta classifier and 4 number of base model is 0.821916\n",
            "F1-Score for Logistic_Regression as Meta classifier and 5 number of base model is 0.823512\n",
            "F1-Score for Logistic_Regression as Meta classifier and 6 number of base model is 0.826029\n",
            "F1-Score for Logistic_Regression as Meta classifier and 7 number of base model is 0.815016\n",
            "F1-Score for Logistic_Regression as Meta classifier and 8 number of base model is 0.825035\n",
            "F1-Score for Logistic_Regression as Meta classifier and 9 number of base model is 0.825035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:478: UserWarning: Converting data to scipy sparse matrix.\n",
            "  warnings.warn('Converting data to scipy sparse matrix.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score for Logistic_Regression as Meta classifier and 10 number of base model is 0.81249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:478: UserWarning: Converting data to scipy sparse matrix.\n",
            "  warnings.warn('Converting data to scipy sparse matrix.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score for Logistic_Regression as Meta classifier and 11 number of base model is 0.817237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uiqTtqw3K79"
      },
      "source": [
        "\n",
        "\n",
        "> From above experiments we can conclude that number of base is 6 gives the highest F1-Score. So we can move ahead with 6 number of base model in our custom stacking classifier.\n",
        "\n",
        "\n"
      ]
    }
  ]
}